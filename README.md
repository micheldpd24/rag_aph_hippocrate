# üèõÔ∏è Assistant Hippocratique : RAG bas√© sur les Aphorismes d'Hippocrate  
> ‚ÄúJe jure par Apollon m√©decin, par Ascl√©pios, Hygie et Panac√©e‚Ä¶‚Äù  
> ‚Äî Serment d‚ÄôHippocrate

[![License](https://img.shields.io/github/license/micheldpd/rag_aph_hippocrate)](LICENSE)

üß† Explorez les fondements de la m√©decine occidentale gr√¢ce √† un syst√®me **RAG (Retrieval-Augmented Generation)** moderne, aliment√© par les *Aphorismes d‚ÄôHippocrate*.

---

## üéØ Objectifs du Projet

- ‚ú® Explorer un corpus historique de textes m√©dicaux anciens.
- üß† Utiliser le **RAG** pour faciliter l‚Äôapprentissage interactif des textes classiques.
- üìö Mettre en avant les principes hippocratiques via une interface num√©rique moderne.
- üßë‚Äç‚öïÔ∏è Inspirer une approche p√©dagogique de la red√©couverte des textes m√©dicaux anciens.

---

## ‚öñÔ∏è Enjeux P√©dagogiques et √âducatifs

### üßë‚Äçüè´ Vulgarisation des textes fondateurs
- Permet aux utilisateurs d‚Äôexplorer les bases de la m√©decine antique.
- Encourage une r√©flexion critique √† travers l‚Äôanalyse des textes originaux.
- Facilite la compr√©hension de l‚Äô√©volution de la pens√©e m√©dicale.

### üìö Apprentissage Interactif
- M√©thode active : poser des questions et recevoir des r√©ponses contextualis√©es.
- D√©veloppe la comp√©tence "recherche + synth√®se" via un assistant IA.
- Favorise l‚Äôautonomie dans l‚Äô√©tude des textes classiques.

### üß† R√©f√©rentiel S√©mantique M√©dical Ancien
- Exploration du langage m√©dical ancien via NLP.
- Premi√®re √©tape vers la cr√©ation de r√©f√©rentiels s√©mantiques historiques.
- Ouvre la voie √† l'analyse d'autres ≈ìuvres majeures (Galien, Avicenne, etc.).

### üí° Exemple Concret d‚Äôun RAG Appliqu√© √† un Corpus de M√©d√©cine Ancienne
- D√©monstration pratique de la cha√Æne compl√®te RAG, du PDF au serveur web.
- Documentation p√©dagogique claire et reproductible.

---

## üóÇ Structure du Projet

```
.
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ hippocrates_questions.json  # Questions pr√©d√©finies
‚îÇ   ‚îî‚îÄ‚îÄ hippocrate_rag_data.json    # Donn√©es pr√©par√©es pour le RAG
‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îî‚îÄ‚îÄ config_schema.py            # Sch√©ma Pydantic de configuration
‚îú‚îÄ‚îÄ prepare_corpus/
‚îÇ   ‚îî‚îÄ‚îÄ process_pdf.ipynb           # Notebook d'extraction du PDF
‚îú‚îÄ‚îÄ rag/
‚îÇ   ‚îî‚îÄ‚îÄ hippocrag.py                # Moteur RAG principal
‚îú‚îÄ‚îÄ templates/
‚îÇ   ‚îî‚îÄ‚îÄ index.html                  # Template Jinja2 de l‚Äôinterface
‚îú‚îÄ‚îÄ app.py                          # Point d‚Äôentr√©e Flask
‚îú‚îÄ‚îÄ config_loader.py                # Chargement du YAML
‚îú‚îÄ‚îÄ Dockerfile                      # D√©finition du conteneur
‚îú‚îÄ‚îÄ docker-compose.yml              # Orchestration
‚îú‚îÄ‚îÄ rag_config.yaml                 # Fichier de configuration global
‚îú‚îÄ‚îÄ requirements.txt                # D√©pendances python pour ex√©cuter le projet dans un conteneur docker
‚îî‚îÄ‚îÄ requirements_pjt.txt            # D√©pendances Python pour executer le projet en local
```

---

## ü§ñ Fonctionnement Technique

### üß† Base de Connaissances
Les *Aphorismes d‚ÄôHippocrate* proviennent de :
- **Titre** : *Aphorismes d'Hippocrate*, traduits par √âmile Littr√©  
- **Format num√©rique** : PDF issu de [archive.org](https://archive.org/details/aphorismesdhippo00hipp)  
- **Pages trait√©es** : 96 √† 260 (traduction fran√ßaise principale)

Ils sont d√©coup√©s et stock√©s dans :
- `data/hippocrate_rag_data.json`  
- Et index√©s avec **FAISS** apr√®s encodage s√©mantique via **Sentence-BERT** (`sentence-camembert-large`).

### üîç Recherche Vectorielle
√Ä chaque question pos√©e :
1. La requ√™te est encod√©e en vecteur.
2. Une recherche par plus proches voisins est effectu√©e dans l‚Äôindex FAISS.
3. Les passages pertinents sont r√©cup√©r√©s.

### üßæ G√©n√©ration de R√©ponse
La r√©ponse est g√©n√©r√©e par un **mod√®le LLM open-source (Mistral)** via **Ollama**, reformul√©e en fran√ßais √† partir des extraits trouv√©s.

### üìä Diagramme du Workflow RAG

```mermaid
graph TD
    A(Question utilisateur) --> B(Encodage vectoriel *CamemBERT*)
    B --> C(Recherche dans l'*index FAISS*)
    C --> D(R√©cup√©ration des chunks pertinents)
    D --> E(Appel LLM *Mistral* ‚Üí r√©ponse g√©n√©r√©e)
```

---

### üßæ Explication d√©taill√©e des √©tapes

1. **[Question utilisateur]**  
   - L'utilisateur pose une question en langage naturel.
   - Exemple : *"Quel est le r√¥le du r√©gime dans la sant√© selon Hippocrate ?"*

2. **[Encodage vectoriel (CamemBERT)]**
   - La question est encod√©e en vecteur gr√¢ce √† un mod√®le s√©mantique comme **CamemBERT** ou **Sentence CamemBERT large**.
   - Permet de comparer la requ√™te aux embeddings pr√©calcul√©s des aphorismes.

3. **[Recherche dans index FAISS]**
   - Le moteur de recherche vectorielle (FAISS) compare le vecteur de la question avec les embeddings stock√©s dans l‚Äôindex.
   - Retourne les `top_k` passages les plus proches (ex: 6 fragments).

4. **[R√©cup√©ration des chunks pertinents]**
   - Les textes associ√©s aux vecteurs trouv√©s sont r√©cup√©r√©s depuis la base JSON.
   - Ces chunks contiennent les informations n√©cessaires pour r√©pondre √† la question.

5. **[Appel LLM (Mistral) ‚Üí r√©ponse g√©n√©r√©e]**
   - Un prompt est construit √† partir de la question et des contextes trouv√©s.
   - Le LLM (`mistral`, via Ollama) g√©n√®re une r√©ponse structur√©e en fran√ßais.
   - R√©ponse renvoy√©e √† l‚Äôutilisateur sous forme HTML ou texte brut.

---

## üß∞ Technologies Utilis√©es

| Technologie       | Usage |
|------------------|-------|
| Flask             | Backend API Web |
| Bootstrap 5.3     | Interface responsive |
| Sentence-BERT     | Encodage s√©mantique |
| FAISS             | Indexation vectorielle |
| Mistral (Ollama)  | G√©n√©ration de r√©ponses |
| Docker            | D√©ploiement portable |
| Bleach            | S√©curisation HTML |

---

## üì¶ Pr√©requis

### ‚úÖ Logiciels requis
- **Python 3.9+**
- **Docker** *(optionnel mais recommand√©)*
- **Ollama** *(ou autre LLM compatible via API)*
- **Git** *(facultatif)*

### üíª Configuration Mat√©rielle Recommand√©e
- **Processeur** : architecture moderne (x86 ou ARM comme M1/M2/M4)
- **RAM** : minimum 16 Go, id√©alement 24 Go
- **Stockage** : espace disque suffisant (~5‚Äì10 Go)

‚úÖ **Test√© avec succ√®s** sur un Mac M4 avec 24 Go de RAM, Python 3.11, Docker Desktop et Ollama.

---

## üöÄ D√©marrage rapide

### 1. üß¨ Cloner le d√©p√¥t distant

Commencez par cloner le d√©p√¥t GitHub sur votre machine locale :

```bash
git clone https://github.com/micheldpd24/rag_aph_hippocrate.git
cd rag_aph_hippocrate
```

---

### 2. üê≥ Avec Docker Compose (Recommand√©)

Assurer vous d'abord que dans le fichier configuration rag_config.yaml nous avons bien:
rag: llm: endpoint: "http://ollama:11434/api/generate"

Lancez l‚Äôapplication en une seule commande gr√¢ce √† Docker Compose :

```bash
docker compose up --build
```

T√©l√©charger et lancer le mod√®le llm mistral
```bash
ollama run mistral
```

L‚Äôapplication sera accessible √† l‚Äôadresse : [http://localhost:5000](http://localhost:5000)

---

### 3. üíª En mode d√©veloppement local

#### a. Pr√©parer l‚Äôenvironnement virtuel

```bash
python -m venv .venv
```

Sous macOS / Linux :
```bash
source .venv/bin/activate
```

Sous Windows (CMD) :
```bash
.venv\Scripts\activate
```

#### b. Installer les d√©pendances Python

```bash
pip install -r requirements.txt
```

#### c. D√©marrer le serveur Ollama

Ouvrez un nouveau terminal et d√©marrez le service Ollama :

```bash
ollama serve
```

#### d. T√©l√©charger le mod√®le LLM (ex. Mistral)

Toujours dans ce terminal :

```bash
ollama run mistral
```

#### e. Configuration du endpoint du llm du RAG:

Assurez vous d'abord que dans le fichier configuration rag_config.yaml vous avez bien:
rag / llm / endpoint: "http://localhost:11434/api/generate"

#### f. Lancer l‚Äôapplication Flask

Revenez au terminal principal et ex√©cutez :

```bash
python app.py
```

Acc√©dez ensuite √† l‚Äôinterface via : [http://localhost:5000](http://localhost:5000)


---

## üìä Extraction & Pr√©paration du Corpus

Le notebook `prepare_corpus/process_pdf.ipynb` extrait les aphorismes depuis le PDF et les pr√©pare pour le RAG :

### üìÑ √âtapes Principales
1. **T√©l√©chargement du PDF**
2. **Nettoyage du texte** (mots coup√©s, erreurs de num√©rotation)
3. **Extraction structur√©e** des aphorismes
4. **Segmentation adapt√©e au RAG** :
   - < 450 caract√®res ‚Üí conserv√© tel quel
   - 450‚Äì800 ‚Üí divis√© en 2 parties
   - > 800 ‚Üí divis√© en 3 parties avec chevauchement

### ‚úÖ Format Final pour le RAG
Chaque document inclut :
- Un identifiant unique (`s1.aph_01.p100`)
- Le texte segment√©
- Des m√©tadonn√©es (section, page, source)

---

## üõ†Ô∏è Configuration du Syst√®me RAG

Fichier central : `rag_config.yaml`

### üñ•Ô∏è Application Flask
| Param√®tre        | Valeur par d√©faut          |
|------------------|----------------------------|
| `name`           | `"HippocRAG"`              |
| `debug`          | `true`                     |
| `host`           | `"0.0.0.0"`                |
| `port`           | `5000`                     |
| `secret_key`     | `"change_this_secret"`     |
| `data_dir`       | `"data"`                   |
| `cache_dir`      | `"cache"`                  |
| `question_cache_file` | `"questions_cache.json"`|

### üîç Moteur RAG
| Param√®tre             | Valeur par d√©faut                        |
|-----------------------|------------------------------------------|
| `model.name`          | `"dangvantuan/sentence-camembert-large"`|
| `normalize_embeddings`| `true`                                   |
| `index.json_path`     | `"data/hippocrate_rag_data.json"`        |
| `faiss_path`          | `"hippocrate.index"`                     |
| `build_on_startup`    | `true`                                   |
| `top_k`               | `6`                                      |

### ü§ñ LLM (Mod√®le de G√©n√©ration)
| Param√®tre   | Valeur par d√©faut / Description                                                                 |
|-------------|--------------------------------------------------------------------------------------------------|
| `provider`  | `"ollama"`<br>‚Üí Fournisseur du mod√®le LLM utilis√©                                               |
| `endpoint`  | - Local : `"http://localhost:11434/api/generate"`<br>‚Üí Si lanc√© manuellement sur la machine<br><br>- Container : `"http://ollama:11434/api/generate"`<br>‚Üí Si lanc√© via Docker Compose |
| `model`     | `"mistral"`<br>‚Üí Mod√®le LLM utilis√© pour g√©n√©rer les r√©ponses                                  |

### üîê S√©curit√©
Balises HTML autoris√©es : `<p>`, `<h4>`, `<ul>`, `<em>`, `<strong>`, `<blockquote>`  
Protection contre XSS via `bleach`.

---

## üß† Impl√©mentation du Moteur RAG

Classe principale : `HippocRAG_FAISS`

### üîß Fonctionnalit√©s
- Encodage s√©mantique via `SentenceTransformer`
- Indexation rapide avec `FAISS`
- G√©n√©ration de r√©ponses via LLM externe
- Configuration flexible via YAML

### üìÅ Entr√©es attendues
- `data/hippocrate_rag_data.json` : donn√©es segment√©es
- `hippocrate.index` : index FAISS pr√©calcul√©

---

## üåê Interface Web

Application Flask interactive permettant :
- De choisir un th√®me m√©dical
- De poser une question personnalis√©e
- D‚Äôobtenir une r√©ponse contextuelle construite √† partir des aphorismes

Routes principales :
| Route            | M√©thode | Description |
|------------------|---------|-------------|
| `/`              | GET/POST| Page principale |
| `/api/questions` | GET     | Charger des questions par th√®me |
| `/reset-cache`   | POST    | Vider le cache des r√©ponses |


Interface web :
![Interface Web - illustration avec un exemple de r√©ponse](img/interface_reponse.png)

---

## üìÑ Licence

MIT License ‚Äì Voir le fichier [LICENSE](LICENSE)

---

## üßë‚Äçüíª Auteur

*Michel Douglas P*  
Ing√©nieur ML. 